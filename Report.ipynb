{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control Report\n",
    "\n",
    "Below is the description of the implementation for this porject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm\n",
    "\n",
    "This project was sobled by making some amendments to the Deep Deterministic Policy Gradients (DDPG) algorithm which comes form Google DeepMind. DDPG is a model-free, off-policy actor-critic algorithm using deep function approximators that can learn policies in high-dimensional, continuous action spaces. It combined the actor-critic approach with insights from the recent success of Deep Q Network (DQN). The network have two parts: Actor network and a Critic network, \n",
    "The Actor network calculate the action based on the current state and the Critic network is used to predict the Q-value. During training, Actor will keep on updating actor policy with the help of Critics feedbacks, Critics will update it's feedback throught Actor's current policy outputs.\n",
    "\n",
    "- In this project, Actor network is build up with 4 fully connected layers,3 hidlen layers be applied leaky_relu activation to increase non_linear characters learning and the last layer applied tanh for a -1 to + 1 output range. Input to the network is state 33 observations from the enviroment, Output is the actions defined by 4 values corresponding to torques applied at the joints.\n",
    "\n",
    "- Critic network is very simlay to the Actor network, 4 fully connected layers,3 hidlen layers be applied leaky_relu activation to increase non_linear character learning. Input the network is enviroment states and output layer will output a signle Q_value.\n",
    "I choosed to use downgread numbers as hidden units activations, 400, 300, 100, from the first hidden layer to the third hidden layer.\n",
    "\n",
    "- The training function named `ddpg()`, which is very similar with Udaicty's pracice sample code, ddpg_agent use a replay buffer which is finit sized cache to store \n",
    "tuples of value which in form of:  `state, action, reward, next_state, done`, at each timestep the actor and critic are updated by sampling a minibatch from buffer. Because DDPG is an off-policy algorithm, the replay buffer can be large, allowing the algorithm to benefit from learning across a set of uncorrelated transitions. In order to decrease overlapping effect of the network, I droped a few number of data from sampled data, so, each sample will be less than minibatch. Agent learning by updating critic and actor which implemented in 'learn()' function, optimizer used Adam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network was trained 150 episode, it starts getting stable at around 60 episode, average score keep above 30 for the rest fo training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for Future Work\n",
    "\n",
    "- Hyperparameters could be further fine tune.\n",
    "- Even the policy is blind here, but the movement of the arms must follow some pysical rules, if we apply suitable pysical theory as constrains during learning, that may boos the learning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
